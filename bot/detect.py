"""
Trigger detection module.

Two-tier detection system:
1. Lemmas (pymorphy3) - primary detection layer
2. Regex patterns - secondary detection layer for variants/evasion

Returns detailed match information for transparency.
"""

import re
from dataclasses import dataclass, asdict
from typing import Optional
from enum import Enum

import pymorphy3

from bot.config import (
    EXCLUSION_PATTERNS,
    RegexRule,
    generate_regex_variants_for_word,
)

# Initialize morphological analyzer
morph = pymorphy3.MorphAnalyzer()

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PERFORMANCE OPTIMIZATION: Pattern Compilation Cache
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Compiled regex patterns are cached to avoid regeneration on every message
# This provides ~2-3ms speedup per message for large trigger sets

_compiled_patterns_cache: dict[str, Optional[re.Pattern]] = {}


class MatchType(str, Enum):
    """Type of match."""
    LEMMA = "lemma"
    REGEX = "regex"


@dataclass
class MatchDetail:
    """Details of trigger match."""
    match_type: MatchType
    original_text: str      # Original text (as in message)
    matched_fragment: str   # Matched fragment
    lemma: Optional[str]    # Lemma (for LEMMA type)
    rule_name: Optional[str]  # Rule name (for REGEX type)
    position_start: int     # Start position in text
    position_end: int       # End position in text
    
    def to_dict(self) -> dict:
        return {
            "match_type": self.match_type.value,
            "original_text": self.original_text,
            "matched_fragment": self.matched_fragment,
            "lemma": self.lemma,
            "rule_name": self.rule_name,
            "position_start": self.position_start,
            "position_end": self.position_end,
        }
    
    def format_human(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é."""
        if self.match_type == MatchType.LEMMA:
            return f'¬´{self.matched_fragment}¬ª (–ª–µ–º–º–∞: {self.lemma})'
        else:
            return f'¬´{self.matched_fragment}¬ª (–ø—Ä–∞–≤–∏–ª–æ: {self.rule_name})'


@dataclass
class DetectionResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏."""
    triggered: bool
    matches: list[MatchDetail]
    excluded: bool = False
    exclusion_reason: Optional[str] = None
    
    def to_dict(self) -> dict:
        return {
            "triggered": self.triggered,
            "matches": [m.to_dict() for m in self.matches],
            "excluded": self.excluded,
            "exclusion_reason": self.exclusion_reason,
        }
    
    @property
    def first_match(self) -> Optional[MatchDetail]:
        """–ü–µ—Ä–≤–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–≥–ª–∞–≤–Ω—ã–π —Ç—Ä–∏–≥–≥–µ—Ä)."""
        return self.matches[0] if self.matches else None


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –ò –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Regex –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: —Å–ª–æ–≤–∞ –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ –∏ –ª–∞—Ç–∏–Ω–∏—Ü–µ
TOKEN_PATTERN = re.compile(r'[–∞-—è—ëa-z]+', re.IGNORECASE)


def normalize_text(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç: lower, —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤."""
    return text.lower().strip()


def tokenize(text: str) -> list[tuple[str, int, int]]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (—Ç–æ–∫–µ–Ω, start, end).
    """
    tokens = []
    for match in TOKEN_PATTERN.finditer(text.lower()):
        tokens.append((match.group(), match.start(), match.end()))
    return tokens


def get_lemma(word: str) -> str:
    """–ü–æ–ª—É—á–∞–µ—Ç –ª–µ–º–º—É —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ pymorphy3."""
    try:
        parsed = morph.parse(word)
        if parsed:
            return parsed[0].normal_form
    except Exception:
        pass
    return word


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ü–†–û–í–ï–†–ö–ê –ò–°–ö–õ–Æ–ß–ï–ù–ò–ô
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def check_exclusions(text: str) -> tuple[bool, Optional[str]]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ–ø–∞–¥–∞–µ—Ç –ª–∏ —Ç–µ–∫—Å—Ç –ø–æ–¥ –∏—Å–∫–ª—é—á–µ–Ω–∏—è.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (excluded, reason).
    """
    normalized = normalize_text(text)
    
    for rule in EXCLUSION_PATTERNS:
        if not rule.enabled:
            continue
        match = rule.pattern.search(normalized)
        if match:
            return True, rule.name
    
    return False, None


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –î–ï–¢–ï–ö–¶–ò–Ø –ü–û –õ–ï–ú–ú–ê–ú
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def detect_by_lemmas(text: str, trigger_lemmas: set[str]) -> list[MatchDetail]:
    """
    –î–µ—Ç–µ–∫—Ü–∏—è –ø–æ –ª–µ–º–º–∞–º.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.
    """
    matches = []
    tokens = tokenize(text)
    
    for token, start, end in tokens:
        lemma = get_lemma(token)
        
        if lemma in trigger_lemmas:
            matches.append(MatchDetail(
                match_type=MatchType.LEMMA,
                original_text=text[start:end],
                matched_fragment=token,
                lemma=lemma,
                rule_name=None,
                position_start=start,
                position_end=end,
            ))
    
    return matches


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# REGEX DETECTION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def get_compiled_pattern(rule_name: str) -> Optional[re.Pattern]:
    """
    Get compiled regex pattern for a rule (with caching).
    
    Performance optimization: Patterns are compiled once and cached.
    This avoids regenerating and recompiling patterns on every message.
    
    Args:
        rule_name: Name of the regex rule (e.g., "–ø—Ä–∏–≤–µ—Ç_spaced")
    
    Returns:
        Compiled regex pattern or None if invalid
    """
    # Check cache first
    if rule_name in _compiled_patterns_cache:
        return _compiled_patterns_cache[rule_name]
    
    # Cache miss - generate and compile pattern
    pattern = None
    
    # Extract base word from rule name (e.g., "–ø—Ä–∏–≤–µ—Ç" from "–ø—Ä–∏–≤–µ—Ç_spaced")
    if '_' in rule_name:
        base_word = rule_name.rsplit('_', 1)[0]
        variants = generate_regex_variants_for_word(base_word)
        
        for variant in variants:
            if variant['name'] == rule_name:
                try:
                    pattern = re.compile(variant['pattern'], re.IGNORECASE | re.UNICODE)
                except re.error:
                    pattern = None
                break
    
    # Cache the result (even if None to avoid repeated lookups)
    _compiled_patterns_cache[rule_name] = pattern
    return pattern


def clear_pattern_cache():
    """Clear the compiled pattern cache (useful for testing or dynamic updates)."""
    global _compiled_patterns_cache
    _compiled_patterns_cache.clear()


def detect_by_regex(text: str, enabled_rules: dict[str, bool]) -> list[MatchDetail]:
    """
    Detection by regex patterns from database.
    Returns list of matches.
    
    Args:
        text: Text to check
        enabled_rules: Dict of rule_name -> enabled status from database
    """
    matches = []
    normalized = normalize_text(text)
    
    # For each enabled rule in database, get compiled pattern and check
    for rule_name, is_enabled in enabled_rules.items():
        if not is_enabled:
            continue
        
        # Get compiled pattern from cache (or compile if first time)
        pattern = get_compiled_pattern(rule_name)
        
        if pattern:
            try:
                for match in pattern.finditer(normalized):
                    matches.append(MatchDetail(
                        match_type=MatchType.REGEX,
                        original_text=text[match.start():match.end()],
                        matched_fragment=match.group(),
                        lemma=None,
                        rule_name=rule_name,
                        position_start=match.start(),
                        position_end=match.end(),
                    ))
            except Exception:
                pass  # Skip patterns that cause runtime errors
    
    return matches


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MAIN DETECTION FUNCTION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def detect_triggers(text: str, trigger_lemmas: set[str], regex_rules_enabled: dict[str, bool]) -> DetectionResult:
    """
    Main trigger detection function.
    
    1. Checks exclusions
    2. Searches by lemmas (primary layer)
    3. Searches by regex (secondary layer)
    4. Returns detailed result
    
    Args:
        text: —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        trigger_lemmas: –Ω–∞–±–æ—Ä –ª–µ–º–º –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏
        regex_rules_enabled: —Å–ª–æ–≤–∞—Ä—å {rule_name: enabled} –¥–ª—è regex-–ø—Ä–∞–≤–∏–ª
    """
    if not text or not text.strip():
        return DetectionResult(triggered=False, matches=[])
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
    excluded, exclusion_reason = check_exclusions(text)
    if excluded:
        return DetectionResult(
            triggered=False,
            matches=[],
            excluded=True,
            exclusion_reason=exclusion_reason,
        )
    
    all_matches = []
    
    # –°–ª–æ–π 1: –ª–µ–º–º—ã
    lemma_matches = detect_by_lemmas(text, trigger_lemmas)
    all_matches.extend(lemma_matches)
    
    # –°–ª–æ–π 2: regex (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ –ª–µ–º–º–∞–º, –∏–ª–∏ –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã)
    regex_matches = detect_by_regex(text, regex_rules_enabled)
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–µ—Å–ª–∏ regex –Ω–∞—à—ë–ª —Ç–æ –∂–µ, —á—Ç–æ –∏ –ª–µ–º–º–∞)
    existing_positions = {(m.position_start, m.position_end) for m in all_matches}
    for rm in regex_matches:
        if (rm.position_start, rm.position_end) not in existing_positions:
            all_matches.append(rm)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏
    all_matches.sort(key=lambda m: m.position_start)
    
    return DetectionResult(
        triggered=len(all_matches) > 0,
        matches=all_matches,
    )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –£–¢–ò–õ–ò–¢–´ –î–õ–Ø –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def format_match_for_message(match: MatchDetail) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è –±–æ—Ç–∞."""
    if match.match_type == MatchType.LEMMA:
        return f'üî§ –°–ª–æ–≤–æ: <b>{match.matched_fragment}</b> ‚Üí –ª–µ–º–º–∞: <code>{match.lemma}</code>'
    else:
        return f'üìù –ü–∞—Ç—Ç–µ—Ä–Ω: <b>{match.matched_fragment}</b> ‚Üí –ø—Ä–∞–≤–∏–ª–æ: <code>{match.rule_name}</code>'


def format_detection_result(result: DetectionResult) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏."""
    if not result.triggered:
        if result.excluded:
            return f"‚ö™ –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {result.exclusion_reason}"
        return "‚úÖ –¢—Ä–∏–≥–≥–µ—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
    
    lines = ["üö® <b>–ù–∞–π–¥–µ–Ω—ã —Ç—Ä–∏–≥–≥–µ—Ä—ã:</b>"]
    for i, match in enumerate(result.matches, 1):
        lines.append(f"  {i}. {format_match_for_message(match)}")
    
    return "\n".join(lines)
